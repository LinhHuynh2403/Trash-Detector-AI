# Experiment Log - Sweep V1: Initial Baseline

**Date:** 2025-11-19
**Goal:** Establish a baseline and determine the optimal epoch count for the frozen ResNet18 feature extractor.
**Command:** `python src/training.py --epochs 10 20 30 --lr 0.0001`
**Reference Data:** `results/training_metrics.csv` (Rows 1-3)

---

## 1. Summary of Results

| Epochs | Final Val Accuracy | Final Val Loss | Best Model File |
| :--- | :--- | :--- | :--- |
| 10 | 69.14% | 1.0740 | `model_e10_lr000100.pth` |
| 20 | 74.07% | 0.8659 | `model_e20_lr000100.pth` |
| **30** | **75.51%** | **0.7790** | `model_e30_lr000100.pth` |

## 2. Analysis: Why 75.51%?

The accuracy of 75.51% is a strong initial result for a baseline using transfer learning.

* **Positive Factor (Transfer Learning):** The high starting accuracy (69.14% at 10 epochs) is due to ResNet18's pre-trained features (edges, textures) being highly relevant to image data, even for garbage.
* **Limiting Factor (Frozen Backbone):** The model is only adjusting its final output layer. The core feature extraction layers are frozen, meaning the model can't learn specific features unique to my dataset (e.g., the specific gloss of plastic vs. metal). This generally caps the maximum achievable accuracy around 75%-85%.
* **Key Insight from Sweep:** Since accuracy and loss improved consistently from 10 to 30 epochs (75.51% is the highest), the model was **underfitting** at 10 and 20 epochs. The $0.0001$ LR and the structure required more time to converge.

## 3. Next Steps & Strategy (Sweep V2)

The goal for the next sweep is to break the 75.51% ceiling by using a smaller LR and initiating **fine-tuning** (unfreezing layers).

### **Strategy: Fine-Tuning Layer 4**

1.  **Modify `utils.py`:** Unfreeze the last convolutional block (`model.layer4`).
2.  **Hypothesize:** A smaller learning rate is critical when unfreezing, to avoid corrupting the pre-trained weights.
3.  **New Command (Sweep V2):** Test a lower range of learning rates around the optimal 30 epochs.

For the next run, try to increase the epoch and decrease the learning rate

```bash
python src/training.py --epochs 30 --lr 0.00005 0.00001